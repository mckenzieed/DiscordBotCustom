var Crawler = require("node-webcrawler");
var url = require('url');
 
var c = new Crawler({
    maxConnections : 10,
    // This will be called for each crawled page 
    callback : function (error, result, $) {
        // $ is Cheerio by default 
        //a lean implementation of core jQuery designed specifically for the server 
        if(error){
            console.log(error);
        }else{
            console.log($("title").text());
        }
    }
});
 

 
// // Queue a list of URLs 
// c.queue(['http://www.google.com/','http://www.yahoo.com']);
 
// // Queue URLs with custom callbacks & parameters 
// c.queue([{
//     uri: 'http://parishackers.org/',
//     jQuery: false,
 
//     // The global callback won't be called 
//     callback: function (error, result) {
//         if(error){
//             console.log(error);
//         }else{
//             console.log('Grabbed', result.body.length, 'bytes');
//         }
//     }
// }]);
 
// // Queue some HTML code directly without grabbing (mostly for tests) 
// c.queue([{
//     html: '<p>This is a <strong>test</strong></p>'
// }]);

module.exports = function (){
    var module = {crawler: c};
    
    module.parser = function(body){
        var jsonObject = JSON.parse(body);
        return jsonObject;
    }

    return module;
}